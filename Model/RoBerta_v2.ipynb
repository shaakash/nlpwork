{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_v2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtZk7yX149HS4Sqjwu8cwD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZ8aZE4cZgUZ","executionInfo":{"status":"ok","timestamp":1622554854142,"user_tz":-330,"elapsed":6757,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"f72b1ca3-2b8c-4293-b722-5df813322cf9"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 29.4MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 40.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 45.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adqRraVAZiSz","executionInfo":{"status":"ok","timestamp":1622554860300,"user_tz":-330,"elapsed":6162,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"02511b3b-9aab-421e-8c6c-e44c89dcde04"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1l8CeexZotP","executionInfo":{"status":"ok","timestamp":1622554863945,"user_tz":-330,"elapsed":3648,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"e584f26e-e827-49b4-8892-7ed6c6bf12ff"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xea00GTRZ8yL","executionInfo":{"status":"ok","timestamp":1622555607137,"user_tz":-330,"elapsed":25035,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"fcc021ef-2ee6-4243-c8fd-7464d4e76c29"},"source":["# Get CoLA dataset from its official source\n","\n","!wget https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n","!unzip cola_public_1.1.zip \n","!cp ./cola_public/raw/in_domain_train.tsv .\n","!cp ./cola_public/raw/out_of_domain_dev.tsv ."],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-06-01 13:53:01--  https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n","Resolving nyu-mll.github.io (nyu-mll.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n","Connecting to nyu-mll.github.io (nyu-mll.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 255330 (249K) [application/zip]\n","Saving to: ‘cola_public_1.1.zip.1’\n","\n","\rcola_public_1.1.zip   0%[                    ]       0  --.-KB/s               \rcola_public_1.1.zip 100%[===================>] 249.35K  --.-KB/s    in 0.006s  \n","\n","2021-06-01 13:53:01 (41.0 MB/s) - ‘cola_public_1.1.zip.1’ saved [255330/255330]\n","\n","Archive:  cola_public_1.1.zip\n","replace cola_public/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace cola_public/tokenized/in_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace cola_public/tokenized/in_domain_train.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace cola_public/tokenized/out_of_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace cola_public/raw/in_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace cola_public/raw/in_domain_train.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace cola_public/raw/out_of_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"2AoTei0FaBxv","executionInfo":{"status":"ok","timestamp":1622555607140,"user_tz":-330,"elapsed":25,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"385ba406-d512-46cf-b037-b2fa2b2b7b24"},"source":["import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n","\n","# Report the number of sentences.\n","print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data.\n","df.sample(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training sentences: 8,551\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_source</th>\n","      <th>label</th>\n","      <th>label_notes</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2389</th>\n","      <td>l-93</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Angela characterized Shelly as a lifesaver.</td>\n","    </tr>\n","    <tr>\n","      <th>5048</th>\n","      <td>ks08</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>They're not finding it a stress being in the s...</td>\n","    </tr>\n","    <tr>\n","      <th>3133</th>\n","      <td>l-93</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>Paul exhaled on Mary.</td>\n","    </tr>\n","    <tr>\n","      <th>5955</th>\n","      <td>c_13</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>I ordered if John drink his beer.</td>\n","    </tr>\n","    <tr>\n","      <th>625</th>\n","      <td>bc01</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Press the stamp against the pad completely.</td>\n","    </tr>\n","    <tr>\n","      <th>3542</th>\n","      <td>ks08</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>They can very.</td>\n","    </tr>\n","    <tr>\n","      <th>6915</th>\n","      <td>m_02</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>This arch is supporting the weight of the tower.</td>\n","    </tr>\n","    <tr>\n","      <th>2908</th>\n","      <td>l-93</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>That new handle detaches easily.</td>\n","    </tr>\n","    <tr>\n","      <th>5857</th>\n","      <td>c_13</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>The Brazilians pumped the oil across the river.</td>\n","    </tr>\n","    <tr>\n","      <th>4191</th>\n","      <td>ks08</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>It is a wooden desk.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     sentence_source  ...                                           sentence\n","2389            l-93  ...        Angela characterized Shelly as a lifesaver.\n","5048            ks08  ...  They're not finding it a stress being in the s...\n","3133            l-93  ...                              Paul exhaled on Mary.\n","5955            c_13  ...                  I ordered if John drink his beer.\n","625             bc01  ...        Press the stamp against the pad completely.\n","3542            ks08  ...                                     They can very.\n","6915            m_02  ...   This arch is supporting the weight of the tower.\n","2908            l-93  ...                   That new handle detaches easily.\n","5857            c_13  ...    The Brazilians pumped the oil across the river.\n","4191            ks08  ...                               It is a wooden desk.\n","\n","[10 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"iXk30fqKaIVY"},"source":["# Get the lists of sentences and their labels.\n","sentences = df.sentence.values\n","labels = df.label.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1yQBSIUaTAN"},"source":["from transformers import RobertaTokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WGUtVo-akcN"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfTXiBBNa49D"},"source":["# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, \n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        padding = 'max_length',\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6f14-Jy8buMr","executionInfo":{"status":"ok","timestamp":1622555610253,"user_tz":-330,"elapsed":14,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"ee1a2133-a242-4fa4-9417-a1f90ffab53b"},"source":["# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[1])\n","print('Token IDs:', input_ids[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  One more pseudo generalization and I'm giving up.\n","Token IDs: tensor([    0,  3762,    55, 38283,   937,  1938,     8,    38,   437,  1311,\n","           62,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhtbiPSbbzdr","executionInfo":{"status":"ok","timestamp":1622555610253,"user_tz":-330,"elapsed":12,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"b78ad3ef-f03d-4f9b-fab5-f99b80e533dc"},"source":["from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7,695 training samples\n","  856 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iW_HP3R3b5br"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cz6cfIkycAv2","executionInfo":{"status":"ok","timestamp":1622555613983,"user_tz":-330,"elapsed":3737,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"1f9df16a-4ea4-4fed-d5eb-3de0b12d8d01"},"source":["from transformers import RobertaForSequenceClassification, AdamW, RobertaConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = RobertaForSequenceClassification.from_pretrained(\n","    \"roberta-large\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"raHVTzi0ch1S"},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eohIaKW_dFqF"},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 8\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PVM25iidMoK"},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6S4F2I3edQRi"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r52uczuvP6Lx","executionInfo":{"status":"ok","timestamp":1622557909368,"user_tz":-330,"elapsed":2295407,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"c1fba45f-2491-48d1-ee72-30153c353afb"},"source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels, return_dict=False)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        total_train_loss += loss.item()\n","\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels, return_dict=False)\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:43.\n","  Batch    80  of    241.    Elapsed: 0:01:27.\n","  Batch   120  of    241.    Elapsed: 0:02:14.\n","  Batch   160  of    241.    Elapsed: 0:03:00.\n","  Batch   200  of    241.    Elapsed: 0:03:47.\n","\n","  Average training loss: 0.51\n","  Training epcoh took: 0:04:34\n","\n","Running Validation...\n","  Accuracy: 0.82\n","  Validation Loss: 0.45\n","  Validation took: 0:00:09\n","\n","======== Epoch 2 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:32.\n","  Batch   120  of    241.    Elapsed: 0:02:19.\n","  Batch   160  of    241.    Elapsed: 0:03:05.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:38.\n","\n","  Average training loss: 0.37\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.83\n","  Validation Loss: 0.39\n","  Validation took: 0:00:09\n","\n","======== Epoch 3 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:33.\n","  Batch   120  of    241.    Elapsed: 0:02:19.\n","  Batch   160  of    241.    Elapsed: 0:03:05.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:38.\n","\n","  Average training loss: 0.27\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.86\n","  Validation Loss: 0.48\n","  Validation took: 0:00:09\n","\n","======== Epoch 4 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:32.\n","  Batch   120  of    241.    Elapsed: 0:02:19.\n","  Batch   160  of    241.    Elapsed: 0:03:05.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:38.\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.88\n","  Validation Loss: 0.37\n","  Validation took: 0:00:09\n","\n","======== Epoch 5 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:32.\n","  Batch   120  of    241.    Elapsed: 0:02:19.\n","  Batch   160  of    241.    Elapsed: 0:03:05.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:37.\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.86\n","  Validation Loss: 0.48\n","  Validation took: 0:00:09\n","\n","======== Epoch 6 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:32.\n","  Batch   120  of    241.    Elapsed: 0:02:19.\n","  Batch   160  of    241.    Elapsed: 0:03:05.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:37.\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation Loss: 0.55\n","  Validation took: 0:00:09\n","\n","======== Epoch 7 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:32.\n","  Batch   120  of    241.    Elapsed: 0:02:19.\n","  Batch   160  of    241.    Elapsed: 0:03:05.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:37.\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation Loss: 0.65\n","  Validation took: 0:00:09\n","\n","======== Epoch 8 / 8 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:46.\n","  Batch    80  of    241.    Elapsed: 0:01:32.\n","  Batch   120  of    241.    Elapsed: 0:02:18.\n","  Batch   160  of    241.    Elapsed: 0:03:04.\n","  Batch   200  of    241.    Elapsed: 0:03:51.\n","  Batch   240  of    241.    Elapsed: 0:04:37.\n","\n","  Average training loss: 0.08\n","  Training epcoh took: 0:04:38\n","\n","Running Validation...\n","  Accuracy: 0.86\n","  Validation Loss: 0.71\n","  Validation took: 0:00:09\n","\n","Training complete!\n","Total training took 0:38:15 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"mcKThwc2dm4t","executionInfo":{"status":"ok","timestamp":1622557909374,"user_tz":-330,"elapsed":64,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"7d9ca325-81c2-42c3-84df-ee5c6f1f1c8e"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.51</td>\n","      <td>0.45</td>\n","      <td>0.82</td>\n","      <td>0:04:34</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.37</td>\n","      <td>0.39</td>\n","      <td>0.83</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.27</td>\n","      <td>0.48</td>\n","      <td>0.86</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.20</td>\n","      <td>0.37</td>\n","      <td>0.88</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.14</td>\n","      <td>0.48</td>\n","      <td>0.86</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.12</td>\n","      <td>0.55</td>\n","      <td>0.87</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.10</td>\n","      <td>0.65</td>\n","      <td>0.87</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.08</td>\n","      <td>0.71</td>\n","      <td>0.86</td>\n","      <td>0:04:38</td>\n","      <td>0:00:09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.51         0.45           0.82       0:04:34         0:00:09\n","2               0.37         0.39           0.83       0:04:38         0:00:09\n","3               0.27         0.48           0.86       0:04:38         0:00:09\n","4               0.20         0.37           0.88       0:04:38         0:00:09\n","5               0.14         0.48           0.86       0:04:38         0:00:09\n","6               0.12         0.55           0.87       0:04:38         0:00:09\n","7               0.10         0.65           0.87       0:04:38         0:00:09\n","8               0.08         0.71           0.86       0:04:38         0:00:09"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfqDGHIRc-l7","executionInfo":{"status":"ok","timestamp":1622557909377,"user_tz":-330,"elapsed":45,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"59fdda57-abc1-42cd-bf56-b8e64d8c0785"},"source":["import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df.sentence.values\n","labels = df.label.values\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        padding='max_length',\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of test sentences: 516\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zx4SohE0x7Yu","executionInfo":{"status":"ok","timestamp":1622557915184,"user_tz":-330,"elapsed":5837,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"cdb2f142-640a-46e1-aae6-74bccc1da931"},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predicting labels for 516 test sentences...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xMEJJGTyJai","executionInfo":{"status":"ok","timestamp":1622557915186,"user_tz":-330,"elapsed":28,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"3d9566a7-8187-484d-ae31-84a5bf4c68fc"},"source":["from sklearn.metrics import matthews_corrcoef\n","from sklearn.metrics import accuracy_score\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Calculating Matthews Corr. Coef. for each batch...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n","  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9DINhzryNL_","executionInfo":{"status":"ok","timestamp":1622557915188,"user_tz":-330,"elapsed":22,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"47f82d24-c09c-48f4-9a9a-33114e14efe9"},"source":["# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total MCC: 0.627\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"id":"KZ1j6bMGyQsL","executionInfo":{"status":"ok","timestamp":1622557916210,"user_tz":-330,"elapsed":33,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"5cea1918-3274-458a-abde-fd3a65c7ab8b"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","import itertools\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion_matrix(cm, title, classes=['Incorrect', 'Correct'],\n","                          cmap=plt.cm.Blues, save=False, saveas=\"MyFigure.png\"):\n","    \n","    # print Confusion matrix with blue gradient colours\n","    \n","    #cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    \n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '0'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('Actual label')\n","    plt.xlabel('Predicted label')\n","    \n","    if save:\n","        plt.savefig(saveas, dpi=100)\n","\n","print(classification_report(flat_true_labels, flat_predictions, target_names=['Incorrect', 'Correct']))\n","print (\"AUC: \", \"{:.1%}\".format(roc_auc_score(flat_true_labels, flat_predictions)))\n","cm = confusion_matrix(flat_true_labels, flat_predictions)\n","plot_confusion_matrix(cm, title=\"Confusion Matrix\", save=True, saveas=\"IF_SA.png\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","   Incorrect       0.87      0.59      0.71       162\n","     Correct       0.84      0.96      0.89       354\n","\n","    accuracy                           0.84       516\n","   macro avg       0.86      0.78      0.80       516\n","weighted avg       0.85      0.84      0.84       516\n","\n","AUC:  77.7%\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"340GRSsiCj_c","executionInfo":{"status":"ok","timestamp":1622558824070,"user_tz":-330,"elapsed":6312,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"660f3b2c-de58-42e1-ba00-fc986b087c3b"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './model_save/'\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","#torch.save(args, os.path.join(output_dir, 'training_args.bin'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to ./model_save/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('./model_save/tokenizer_config.json',\n"," './model_save/special_tokens_map.json',\n"," './model_save/vocab.json',\n"," './model_save/merges.txt',\n"," './model_save/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzOy_k4JJHJV","executionInfo":{"status":"ok","timestamp":1622560667846,"user_tz":-330,"elapsed":39126,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"40bd3403-6e67-4365-a638-1a5c0c778c9d"},"source":["# Mount Google Drive to this Notebook instance.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WCygz9PWJHK_"},"source":["# Copy the model files to a directory in your Google Drive.\n","!cp -r ./model_save/ \"./drive/MyDrive/Roberta/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmdFMLpbCsot"},"source":["def grammar_check(sent):\n","  output_dir = './model_save/'\n","  #output_dir = './drive/MyDrive/Roberta/'\n","  tokenizer = RobertaTokenizer.from_pretrained(output_dir)\n","  model_loaded = RobertaForSequenceClassification.from_pretrained(output_dir)\n","\n","  encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        padding='max_length',\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","  input_id = encoded_dict['input_ids']\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","  attention_mask = encoded_dict['attention_mask']\n","  input_id = torch.LongTensor(input_id)\n","  attention_mask = torch.LongTensor(attention_mask)\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model_loaded = model_loaded.to(device)\n","  input_id = input_id.to(device)\n","  attention_mask = attention_mask.to(device)\n","\n","  with torch.no_grad():\n","  # Forward pass, calculate logit predictions\n","    outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n","\n","  logits = outputs[0]\n","  index = logits.argmax()\n","\n","  if index == 1:\n","    return (\"Gramatically correct\")\n","  else:\n","    return (\"Gramatically in-correct\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEDbjISYDVts","executionInfo":{"status":"ok","timestamp":1622559375918,"user_tz":-330,"elapsed":4660,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"8a6c6987-c9de-4a45-8f81-6d9101d3bb14"},"source":["# sent = \"I is eating cake.\"\n","# output_dir = './model_save/'\n","# tokenizer = RobertaTokenizer.from_pretrained(output_dir)\n","# model_loaded = RobertaForSequenceClassification.from_pretrained(output_dir)\n","\n","# encoded_dict = tokenizer.encode_plus(\n","#                         sent,                      # Sentence to encode.\n","#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","#                         max_length = 64,           # Pad & truncate all sentences.\n","#                         padding='max_length',\n","#                         return_attention_mask = True,   # Construct attn. masks.\n","#                         return_tensors = 'pt',     # Return pytorch tensors.\n","#                    )\n","    \n","#     # Add the encoded sentence to the list.    \n","# input_id = encoded_dict['input_ids']\n","    \n","#     # And its attention mask (simply differentiates padding from non-padding).\n","# attention_mask = encoded_dict['attention_mask']\n","# input_id = torch.LongTensor(input_id)\n","# attention_mask = torch.LongTensor(attention_mask)\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model_loaded = model_loaded.to(device)\n","# input_id = input_id.to(device)\n","# attention_mask = attention_mask.to(device)\n","\n","# with torch.no_grad():\n","\n","#   # Forward pass, calculate logit predictions\n","#   outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n","\n","# logits = outputs[0]\n","# index = logits.argmax()\n","\n","# if index == 1:\n","#   print (\"Gramatically correct\")\n","# else:\n","#   print (\"Gramatically in-correct\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Gramatically in-correct\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kt9lJsB2E_hY","executionInfo":{"status":"ok","timestamp":1622559444780,"user_tz":-330,"elapsed":515,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"e6a99eba-c469-40e9-d5f3-d9ce6b3fd77c"},"source":["# logits"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.7112, -3.0666]], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJKW8G9MFJS6","executionInfo":{"status":"ok","timestamp":1622559467245,"user_tz":-330,"elapsed":669,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"bee46b3e-489e-4199-edd7-bc666cdeb604"},"source":["# index"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0, device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZqzH3EWmGo-P","executionInfo":{"status":"ok","timestamp":1622559879107,"user_tz":-330,"elapsed":9477,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"deb61e7d-8716-4886-b403-46ffc86d3955"},"source":["!pip install gradio"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting gradio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/d3/259cdc42333db00a06a06699d584ff5797c7efed8bee4d11c4d26e154a11/gradio-2.0.1-py3-none-any.whl (1.4MB)\n","\r\u001b[K     |▎                               | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 22.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 24.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 25.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 27.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 24.4MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 25.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 26.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102kB 25.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112kB 25.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122kB 25.0MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 25.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143kB 25.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153kB 25.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163kB 25.0MB/s eta 0:00:01\r\u001b[K     |████                            | 174kB 25.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184kB 25.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194kB 25.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 204kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 215kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 225kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 235kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 245kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 256kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 266kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 276kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 286kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 296kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 307kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 317kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 327kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 337kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 348kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 358kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 368kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 378kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 399kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 409kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 419kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 430kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 440kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 450kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 460kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 471kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 481kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 491kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 501kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 512kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 522kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 532kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 542kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 552kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 563kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 573kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 583kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 593kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 604kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 614kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 624kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 634kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 645kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 655kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 665kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 675kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 686kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 696kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 706kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 716kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 727kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 737kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 747kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 757kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 768kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 778kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 788kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 798kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 808kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 819kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 829kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 839kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 849kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 860kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 870kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 880kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 890kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 901kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 911kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 921kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 931kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 942kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 952kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 962kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 972kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 983kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 993kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.0MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.0MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 25.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.19.5)\n","Collecting ffmpy\n","  Downloading https://files.pythonhosted.org/packages/bf/e2/947df4b3d666bfdd2b0c6355d215c45d2d40f929451cb29a8a2995b29788/ffmpy-0.3.0.tar.gz\n","Requirement already satisfied: Flask>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from gradio) (1.1.4)\n","Collecting analytics-python\n","  Downloading https://files.pythonhosted.org/packages/30/81/2f447982f8d5dec5b56c10ca9ac53e5de2b2e9e2bdf7e091a05731f21379/analytics_python-1.3.1-py2.py3-none-any.whl\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n","Collecting flask-cachebuster\n","  Downloading https://files.pythonhosted.org/packages/74/47/f3e1fedfaad965c81c2f17234636d72f71450f1b4522ca26d2b7eb4a0a74/Flask-CacheBuster-1.0.0.tar.gz\n","Collecting Flask-Cors>=3.0.8\n","  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n","Collecting markdown2\n","  Downloading https://files.pythonhosted.org/packages/5d/be/3924cc1c0e12030b5225de2b4521f1dc729730773861475de26be64a0d2b/markdown2-2.4.0-py2.py3-none-any.whl\n","Collecting pycryptodome\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 48.2MB/s \n","\u001b[?25hCollecting paramiko\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n","\u001b[K     |████████████████████████████████| 215kB 51.9MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.1.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.4.1)\n","Collecting Flask-Login\n","  Downloading https://files.pythonhosted.org/packages/2b/83/ac5bf3279f969704fc1e63f050c50e10985e50fd340e6069ec7e09df5442/Flask_Login-0.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.15.0)\n","Collecting monotonic>=1.5\n","  Downloading https://files.pythonhosted.org/packages/9a/67/7e8406a29b6c45be7af7740456f7f37025f0506ae2e05fb9009a53946860/monotonic-1.6-py2.py3-none-any.whl\n","Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.1)\n","Collecting backoff==1.10.0\n","  Downloading https://files.pythonhosted.org/packages/f0/32/c5dd4f4b0746e9ec05ace2a5045c1fc375ae67ee94355344ad6c7005fd87/backoff-1.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n","Collecting cryptography>=2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 49.3MB/s \n","\u001b[?25hCollecting pynacl>=1.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n","\u001b[K     |████████████████████████████████| 962kB 48.0MB/s \n","\u001b[?25hCollecting bcrypt>=3.1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2018.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.4.7)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.1.1->gradio) (2.0.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.14.5)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.20)\n","Building wheels for collected packages: ffmpy, flask-cachebuster\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.0-cp37-none-any.whl size=4710 sha256=450b0507ecd361778046475afac076026909fe5bb19a133260e9552c9a192481\n","  Stored in directory: /root/.cache/pip/wheels/cc/ac/c4/bef572cb7e52bfca170046f567e64858632daf77e0f34e5a74\n","  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flask-cachebuster: filename=Flask_CacheBuster-1.0.0-cp37-none-any.whl size=3372 sha256=c4777291d20a6e62d1ec652f5cc2a8f45be8f22308e8abe189babbd635be5156\n","  Stored in directory: /root/.cache/pip/wheels/9f/fc/a7/ab5712c3ace9a8f97276465cc2937316ab8063c1fea488ea77\n","Successfully built ffmpy flask-cachebuster\n","Installing collected packages: ffmpy, monotonic, backoff, analytics-python, flask-cachebuster, Flask-Cors, markdown2, pycryptodome, cryptography, pynacl, bcrypt, paramiko, Flask-Login, gradio\n","Successfully installed Flask-Cors-3.0.10 Flask-Login-0.5.0 analytics-python-1.3.1 backoff-1.10.0 bcrypt-3.2.0 cryptography-3.4.7 ffmpy-0.3.0 flask-cachebuster-1.0.0 gradio-2.0.1 markdown2-2.4.0 monotonic-1.6 paramiko-2.7.2 pycryptodome-3.10.1 pynacl-1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":640},"id":"LVw8MJF9Gs1B","executionInfo":{"status":"ok","timestamp":1622559921829,"user_tz":-330,"elapsed":3641,"user":{"displayName":"Aakash Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgY1U43tf9_Z6VZV1hsLBf_u9Guj9GqlyzZB8ICsw=s64","userId":"03793056575879410905"}},"outputId":"42a8d1c7-5998-4f68-f726-2e6cbf5f9e82"},"source":["import gradio as gr\n","iface=gr.Interface(grammar_check, inputs=\"text\", outputs=\"text\")\n","iface.launch()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n","This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n","Running on External URL: https://48175.gradio.app\n","Interface loading below...\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","        <iframe\n","            width=\"900\"\n","            height=\"500\"\n","            src=\"https://48175.gradio.app\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7fe0e75ba150>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["(<Flask 'gradio.networking'>,\n"," 'http://127.0.0.1:7860/',\n"," 'https://48175.gradio.app')"]},"metadata":{"tags":[]},"execution_count":51}]}]}